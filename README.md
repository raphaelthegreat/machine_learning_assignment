## Στόχοι

Στόχος αυτής της εργασίας ήταν η βελτιστοποίηση ενός μοντέλου αναγνώρισης χειρόγραφων αριθμών, για το οποίο μας δόθηκε μια αρχική υλοποίηση.
Η βελτιστοποίηση έγινε με hyperparameter tuning, όπου πειραματίστηκα με τις ρυθμίσεις του υπάρχοντος μοντέλου με σκοπό να αυξηθεί η ακρίβεια του
και με γενικότες αλλαγές στα δεδομένα εισόδου και στην αρχιτεκτονική του.

## Hyperparameter tuning

Αρχικά στην πρώτη κατηγορία πειραματίστηκα με διάφορα epochs, αριθμό κρυφών επιπέδων και unit μέσα στα επίπεδα αυτά. Γενικά τα αποτέλεσματα έδειξαν ότι όσο αυξάνεται το epoch τόσο αυξάνεται και το accuracy.
Επίσης τα units των κρυφών επιπέδων είχαν μια σημαντική επίδραση στο accuracy και λιγότερη, αλλά πάλι υπαρκτή, μια αύξηση των κρυφών επιπέδων. Κατέληξα στα 20 epochs με 4 κρυφά επίπεδα απο 512 units.
Ωστόσο μια αναζήτηση που έκανα δηλώνει πως 50 ίσως είναι καλύτερα αλλά για αυτό το πείραμα η τιμή αυτή ήταν απαγορευτική υπολογιστικά, άρα δεν πήγα τόσο ψηλά (https://datascience.stackexchange.com/a/106273)

Πειράματα με το learning rate έδειξαν ότι 0.003 ήταν η καλύτερη τίμη. Πιο μεγάλες τιμές όπως 0.005 οδήγησαν σε μείωση του accuracy (έχω τίμες σε πινακάκι στο ipynb)
Για activation function στα κρυφά επίπεδα softplus και relu είχαν πολύ μεγάλο accuracy (0.9999 για το πρώτο) ενώ το softmax ήταν πολύ χαμηλό (δεν είμαι 100% σίγουρος για τα αποτελέσματα η διαφορά είναι λίγο δραματική)

# Improvements

Κάνοντας μια γρήγορη αναζήτηση στο ιντερνετ βρήκα αυτό το άρθρο https://nagadakos.github.io/2018/09/23/dropout-effect-discussion/ που πρότεινε τη χρήση Dropout επιπέδων για αποφυγή overfitting,
άρα ήταν η πρώτη βελτίωση που τέσταρα.

Επίσης βρήκα και τη μέθοδο του batch normalization (https://medium.com/@anderaquerretamontoro/99-46-accuracy-on-mnist-without-cnn-712042530420) όπου η έξοδος κάθε επιπέδου του δικτύου κανονικοποιείται
πριν την τροφοδοσία της στο επόμενο επίπεδο. Μόνο αυτή η αλλαγή είχε καλή βελτίωση στο accuracy φέρνοντας το κοντά στο 0.9932. Για την υλοποίηση πρόσθεσα ένα tf.keras.layers.Flatten(input_shape=(28, 28)), layer μετά το input και tf.keras.layers.BatchNormalization() μετα απο κάθε hidden layer για την κανονικοποίηση.


Εκτός από 

### New hyperparameters

Learning Rate = 0.003
Epochs = 20
Activation = softplus
Dropout = 0.25

## Questions

Θεωρείτε πως τα δεδομένα της MNIST είναι καλά για την εκπαίδευση ενός μοντέλου?
Αιτιολογείστε.

- Ναι, τα δοθέντα δεδομένα είναι αρκετά και έχουν διαφορές/μοναδικότητα μεταξύ τους που περιορίζει το memorization

Θεωρείτε πως όλα τα pixel είναι σημαντικά για την πρόβλεψη της κλάσης ενός
ψηφίου

- Κάθε αριθμός όριζεται απο τα άσπρα pixel, άρα τα μαύρα είναι σχεδόν άχρηστα

Σε ποιες περιπτώσεις είναι καλή ιδέα να χρησιμοποιηθούν Βαθιά Νευρωνικά Δίκτυα?

- Όταν η ομαδοποίηση των δεδομένων έχει υπερβολικά πολλές παραμέτρους που είναι δύσκολο να γραφτεί κώδικας για την εργασία αυτή.
  Επίσης όταν τα δεδομένα είναι πιο οπτικής φύσεως είναι αρκετά δύσκολο το παραδοσιακό parsing και "κατανόηση" τους.

  Η Βαθιά Μάθηση μπορεί να χρησιμοποιηθεί και στους 3 κλάδους της Μηχανικής
Μάθησης? (Supervised Learning, Unsupervised Learning και Reinforcement
Learning)

- Ναι φυσικά
